# cs224-lecture11-ConvNet

Created By: Geunho Lee
Last Edited: Jul 19, 2020 3:47 PM

## From RNNs to Convolutional Neural Nets

![cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled.png](cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled.png)

- 주로 마지막 단어 벡터에 많은 가중치가 실림
- n-gram 처럼 특정 길이의 subsequence를 다 보자 → CNN

## Intro CNN

![cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%201.png](cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%201.png)

![cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%202.png](cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%202.png)

- 문장 제일 앞/뒤에 패딩 넣어서 길이 유지하는 케이스

![cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%203.png](cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%203.png)

- filter (또는 kernel)을 여러개 쓰고 pooling 하는 예제
- max pooling 외에 avg pooling 등도 있음
- 그 외 stride 조정이나 local pooling, dilation 등도 있지만 별로 중요한 것 같지 않아서 스킵

## Single Layer CNN for Sentence Classification

Yoon Kim (2014): Convolutional Neural Networks for Sentence Classification. EMNLP

Sentence classification (Positive / Negative)

아래 그림으로 설명된다

![cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%204.png](cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%204.png)

- ReLU
- filter size = 3, 4, 5
- 100 feature maps
- Drop out p = 0.5 (2~4% improvement)
- L2 constraints
- 50 batch size
- pretrained word2vec, k = 300

![cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%205.png](cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%205.png)

- 이전 논문들은 dropout 사용하지 않은 것도 있기 때문에 평가가 공정하지 않다고 함
- 뭐 그래도 성능이 나쁘진 않음 ㅎㅎ

### 우리가 가지고 있는 무기들

![cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%206.png](cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%206.png)

### Gated units used vertically

![cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%207.png](cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%207.png)

### Batch normalization

- LayerNorm (standard in Transformers)

### Size 1 Convolutions

- Map from many channels to fewer channels
- 즉 파라미터를 줄이는 역할이 가능함

## Apps

![cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%208.png](cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%208.png)

![cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%209.png](cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%209.png)

![cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%2010.png](cs224%20lecture11%20ConvNet%20ed6a1252d7a94f5eb050696963698884/Untitled%2010.png)