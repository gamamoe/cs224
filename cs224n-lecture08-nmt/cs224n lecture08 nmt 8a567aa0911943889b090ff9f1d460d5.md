# cs224n-lecture08-nmt

Created By: Geunho Lee
Last Edited: Jun 21, 2020 2:21 PM

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled.png)

# Section 1: Pre-Neural Machine Translation

문장 $x$ (source language) to 문장 $y$ (target language)

$x$: L'homme est ne libre, et partout il est dans les fers

$y$: Man is born free, but everywhere he is in chains

번역이므로 chains, irons 등 다양하게 가능

## 1950s: Early Machine Translation

주로 Russian → English (냉전 시기)

단순히 Rule-based로 러시아 단어를 영어 대치 단어로 옮기는 시스템

## 1990s-2010s: Statistical Machine Translation

데이터로부터 probabilistic model 학습

즉, given source sentence $x$로부터 가장 best인 target sentence $y$ 를 찾기를 원함

$$argmax_{y}P(y|x)$$

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%201.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%201.png)

Bayes 룰 활용하여 두 개의 컴포넌트로 분해 가능

- 번역 모델
- 언어 모델

이런 작업을 위해서는 대용량의 parallel data가 필요함 (e.g. 번역된 프랑스어/영어 문장들)

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%202.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%202.png)

위 수식을 좀 더 확장하여 alignment 개념도 들어가야함

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%203.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%203.png)

- 대치 단어가 없는 경우
- 여러 단어가 하나의 단어로 (many-to-one)
- 한 단어가 여러개의 단어로 (one-to-many)
- 여러 단어가 여러개의 단어로 (many-to-many)

잠재 변수로 놓고 EM 등으로 해당 변수 학습한다고 하는데, 여기서는 자세히 안 다룸

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%204.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%204.png)

### Decoding for SMT

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%205.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%205.png)

모든 가능한 y에 대해서 다 계산하는 것은 너무 연산량이 많음, Viterbi Dynamic programming으로 글로벌 최적 구함 → 이 과정을 decoding

SMT는 너무 복잡함

분리되어 디자인된 서브 컴포넌트들이 많이 존재하고, 많은 피쳐 엔지니어링 공수가 필요하고, 유지보수하는데 사람의 리소스가 너무 많이 듬

# Section 2: Neural Machine Translation

기계 번역을 하나의 신경망으로 하는 것 (End-to-End로), sequence-to-sequence로 불리며 두 개의 RNN으로 구성됨

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%206.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%206.png)

단순히 기계 번역 뿐만 아니라 여러 분야에 응용 가능

- Summarization (long text → short text)
- Dialogue (previous utterances → next utterance)
- Parsing (input text → output parse as sequence)
- Code generation (natual language → Python code)

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%207.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%207.png)

target sentence의 다음 단어를 예측 → language model

source sentence given에 대해서 prediction → conditional

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%208.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%208.png)

위와 같이 디코딩 하는 방식을 greedy decoding이라고 함 (각 스텝마다 가장 그럴싸한 것 고르는 것)

이 방법은 무슨 문제가 있음?

ㄴ 그리디 방식은 글로벌 옵티멈을 보장하지 않음, 즉

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%209.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%209.png)

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2010.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2010.png)

모든 가능한 y에 대해서 하는 것은 너무 계산량이 큼!

항상 최적 솔루션을 보장하진 않지만, 이걸 보완하는 Beam search decoding을 활용

→ 각 스텝마다 k most probable partial translations (also call hypothesis)

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2011.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2011.png)

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2012.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2012.png)

보통은 <END> token 생성까지 디코딩함

서로 다른 ts에서 <END> 발생하면? → 우선 끝난 애들은 따로 저장

보통 1. pre-defined ts cutoff 까지 또는 2. at least n completed hypothesis 일 동안 디코딩

확률의 곱으로 계속 이어지므로 (log sum) length normalization 해줌

$${1 \over t} \sum_{i=1}^{t}logP_{LM}(y_{i}|y_{1},...,y_{i-1},x)$$

## NMT의 장점

- Better Performance
    - 더 유창하고
    - 더 컨텍스트를 잘 이용하고
    - 더 구문 유사도를 잘 활용함
- 단일 뉴럴넷으로 end-to-end 최적화
    - No subcomponents 필요
- 휴먼 리소스 덜 필요함
    - No feature engineering
    - 모든 언어 페어쌍에 대해서 같은 방법

## NMT의 단점 (Compared to SMT)

- Less interpretable
    - 디버깅 어려움
- Difficult to control
    - 특정 룰이나 가이드라인 적용이나
    - Safety concerns! (F-word나 인종차별 등?)

## 기계 번역 평가 → BLEU (Bilingual Evaluation Understudy)

기계 번역된 결과를 하나 또는 그 이상의 인간 번역과 유사도 비교

- n-gram precision (1, 2, 3 and 4-grams)
- 추가로 너무 짧게 번역된 것에 대한 페널티

완벽한 메트릭은 아님

기계가 잘 번역했지만, 나쁜 BLEU 스코어 받을 수 있음 (n-gram overlap이 낮은 경우)

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2013.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2013.png)

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2014.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2014.png)

## 그래서 MT가 해결됨?

Nope!

- Out-of-vocab 문제
- Domain mismatch between train and test
- Maintaining context over longer text
- Low-resource language pairs

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2015.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2015.png)

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2016.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2016.png)

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2017.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2017.png)

# Section 3: Attention

Vanilla seq2seq에서 Attention으로 대동단결!

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2018.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2018.png)

기존 seq2seq은 마지막 단어의 single vector가 인코더의 모든 정보 표현해야하는 bottleneck 문제

## Attention

디코더의 각 스텝에서 인코더와 direct connection을 맺어서 source sequence의 특정 부분에 집중

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2019.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2019.png)

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2020.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2020.png)

수식으로 풀어쓰면

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2021.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2021.png)

### Attention 장점

- NMT 성능 significantly 향상
- bottleneck 문제 해결
- vanishing gradient problem 완화
- 해석가능성 제공

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2022.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2022.png)

### Attention variants

![cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2023.png](cs224n%20lecture08%20nmt%208a567aa0911943889b090ff9f1d460d5/Untitled%2023.png)